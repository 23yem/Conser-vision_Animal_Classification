{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to my notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default code from Kaggle Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "   \n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying some important libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 21:25:16.555008: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-01 21:25:16.901198: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-01 21:25:16.901266: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-01 21:25:16.962174: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-01 21:25:17.084277: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-01 21:25:18.280355: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow: 2.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_541/2944158885.py:4: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  import kerastuner as kt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kerastuner: 1.0.5\n",
      "keras_tuner: 1.3.5\n",
      "Python: 3.10.12\n",
      "numpy: 1.24.3\n",
      "pandas: 2.1.4\n",
      "sklearn version: 1.2.2\n",
      "sklearn path: ['/home/michaelye22/.local/lib/python3.10/site-packages/sklearn']\n",
      "matplotlib: 3.8.2\n",
      "seaborn: 0.13.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow:\", tf.__version__)\n",
    "\n",
    "import kerastuner as kt\n",
    "print(\"kerastuner:\", kt.__version__)\n",
    "\n",
    "import keras_tuner as kt2\n",
    "print(\"keras_tuner:\", kt2.__version__)\n",
    "\n",
    "import platform\n",
    "print(\"Python:\", platform.python_version())\n",
    "\n",
    "import numpy as np\n",
    "print(\"numpy:\", np.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "print(\"pandas:\", pd.__version__)\n",
    "\n",
    "import sklearn\n",
    "print(\"sklearn version:\", sklearn.__version__)\n",
    "\n",
    "import sklearn\n",
    "print(\"sklearn path:\", sklearn.__path__)\n",
    "\n",
    "import matplotlib\n",
    "print(\"matplotlib:\", matplotlib.__version__)\n",
    "\n",
    "import seaborn as sns\n",
    "print(\"seaborn:\", sns.__version__)\n",
    "\n",
    "# On WSL\n",
    "\n",
    "# 2024-01-30 11:17:52.768682: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
    "# 2024-01-30 11:17:53.149956: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
    "# 2024-01-30 11:17:53.150001: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
    "# 2024-01-30 11:17:53.210606: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
    "# 2024-01-30 11:17:53.339576: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
    "# To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
    "# 2024-01-30 11:17:54.568146: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
    "# Tensorflow: 2.15.0\n",
    "# /tmp/ipykernel_3814/2917868046.py:4: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
    "#   import kerastuner as kt\n",
    "# kerastuner: 1.0.5\n",
    "# keras_tuner: 1.3.5\n",
    "# Python: 3.10.12\n",
    "# numpy: 1.24.3\n",
    "# pandas: 2.1.4\n",
    "# sklearn version: 1.2.2\n",
    "# sklearn path: ['/home/michaelye22/.local/lib/python3.10/site-packages/sklearn']\n",
    "# matplotlib: 3.8.2\n",
    "# seaborn: 0.13.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Global random seed to make sure we can replicate any model that we create (no randomness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Preprocessing:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the training and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_values are the features (X), and train_labels is the target/label (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.read_csv(\"train_features.csv\")\n",
    "train_y = pd.read_csv(\"train_labels.csv\")\n",
    "\n",
    "test_values = pd.read_csv(\"test_features.csv\")\n",
    "\n",
    "# print(\"train labels:\\n\", train_Y.head())\n",
    "\n",
    "# print(\"train values:\\n\", train_X.head())\n",
    "      \n",
    "# print(\"test_values:\\n\", test_values.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check to see if there are any missing values in the data. If so, we have to do imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in train_X: 0\n",
      "Number of missing values in train_Y: 0\n",
      "Number of missing values in test_values: 0\n"
     ]
    }
   ],
   "source": [
    "missing_train_X = train_X.isnull().sum().sum()\n",
    "print(\"Number of missing values in train_X:\", missing_train_X)\n",
    "\n",
    "missing_train_y = train_y.isnull().sum().sum()\n",
    "print(\"Number of missing values in train_Y:\", missing_train_y)\n",
    "\n",
    "missing_test_values = test_values.isnull().sum().sum()\n",
    "print(\"Number of missing values in test_values:\", missing_test_values )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have 0 missing values in each dataframe, we don't have to do imputation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified train_test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Train Distribution  Test Distribution\n",
      "site_animal                                                  \n",
      "S0001_bird                        0.000647           0.000728\n",
      "S0001_blank                       0.000404           0.000243\n",
      "S0001_leopard                     0.003073           0.003153\n",
      "S0001_monkey_prosimian            0.001051           0.000970\n",
      "S0002_bird                        0.000566           0.000485\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the datasets\n",
    "\n",
    "train_features = train_X\n",
    "train_labels = train_y\n",
    "\n",
    "# Merge the datasets on 'id'\n",
    "merged_data = pd.merge(train_features, train_labels, on='id')\n",
    "\n",
    "# Identifying the animal present in each image and creating a combined category\n",
    "animal_columns = ['antelope_duiker', 'bird', 'blank', 'civet_genet', 'hog', 'leopard', 'monkey_prosimian', 'rodent']\n",
    "merged_data['animal'] = merged_data[animal_columns].idxmax(axis=1)\n",
    "merged_data['site_animal'] = merged_data['site'] + '_' + merged_data['animal']\n",
    "\n",
    "# Checking the number of instances for each site_animal combination\n",
    "combination_counts = merged_data['site_animal'].value_counts()\n",
    "rare_combinations = combination_counts[combination_counts < 5]\n",
    "\n",
    "# Separating the dataset into common and rare combinations\n",
    "common_combinations = merged_data[~merged_data['site_animal'].isin(rare_combinations.index)]\n",
    "rare_combinations_data = merged_data[merged_data['site_animal'].isin(rare_combinations.index)]\n",
    "\n",
    "# Stratified split for common combinations\n",
    "common_train_set, common_test_set = train_test_split(\n",
    "    common_combinations, test_size=0.25, stratify=common_combinations['site_animal'], random_state=42)\n",
    "\n",
    "# Randomly splitting rare combinations\n",
    "total_samples = rare_combinations_data.shape[0]\n",
    "train_samples = int(np.round(total_samples * 0.75))\n",
    "rare_train_set = rare_combinations_data.sample(n=train_samples, random_state=42)\n",
    "rare_test_set = rare_combinations_data.drop(rare_train_set.index)\n",
    "\n",
    "# Combining the splits into final train and test sets\n",
    "final_train_set = pd.concat([common_train_set, rare_train_set])\n",
    "final_test_set = pd.concat([common_test_set, rare_test_set])\n",
    "\n",
    "# Optional: Verifying the final distribution (can be commented out for large datasets)\n",
    "final_train_distribution = final_train_set['site_animal'].value_counts(normalize=True)\n",
    "final_test_distribution = final_test_set['site_animal'].value_counts(normalize=True)\n",
    "final_distribution_summary = pd.DataFrame({\n",
    "    'Train Distribution': final_train_distribution,\n",
    "    'Test Distribution': final_test_distribution\n",
    "})\n",
    "print(final_distribution_summary.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the 75/25 train_test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Proportion: 74.99%\n",
      "Test Set Proportion: 25.01%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of samples in each set\n",
    "num_train_samples = final_train_set.shape[0]\n",
    "num_test_samples = final_test_set.shape[0]\n",
    "total_samples = num_train_samples + num_test_samples\n",
    "\n",
    "# Calculate the proportions\n",
    "train_proportion = num_train_samples / total_samples\n",
    "test_proportion = num_test_samples / total_samples\n",
    "\n",
    "# Print out the proportions\n",
    "print(\"Training Set Proportion: {:.2%}\".format(train_proportion))\n",
    "print(\"Test Set Proportion: {:.2%}\".format(test_proportion))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making sure that train and test set have a 75/25 split for each site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, for each site, around 75% should be in the training data and around 25% should be in the test data. This ensures niether the train or test set have a unbalanced amount of a certain site, leading to bias and bad predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the count of each site in both sets\n",
    "site_counts_train = final_train_set['site'].value_counts()\n",
    "site_counts_test = final_test_set['site'].value_counts()\n",
    "\n",
    "# Combine the counts into a single DataFrame for comparison\n",
    "combined_site_counts = pd.DataFrame({'Train Count': site_counts_train, 'Test Count': site_counts_test})\n",
    "\n",
    "# Calculate the total counts for each site\n",
    "combined_site_counts['Total Count'] = combined_site_counts['Train Count'] + combined_site_counts['Test Count']\n",
    "\n",
    "# Calculate the percentage split for each site\n",
    "combined_site_counts['Train Percentage'] = (combined_site_counts['Train Count'] / combined_site_counts['Total Count']) * 100\n",
    "combined_site_counts['Test Percentage'] = (combined_site_counts['Test Count'] / combined_site_counts['Total Count']) * 100\n",
    "\n",
    "# Display the combined counts with percentage split\n",
    "#print(combined_site_counts.head())\n",
    "combined_site_counts.to_csv(\"site_percentage_check.csv\")\n",
    "\n",
    "# Check to see if there any rows of data with a train percentage below 70 or above 80 (the ideal is 75)\n",
    "filtered_df = combined_site_counts[(combined_site_counts['Train Percentage'] < 70) | (combined_site_counts['Train Percentage'] > 80)]\n",
    "#print(filtered_df)\n",
    "#print(len(filtered_df)) # There are only 13 sites which have a bad train/test split, but they all side more towards the train set, which is good\n",
    "\n",
    "\n",
    "#print(len(final_train_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 13 sites which have a bad train/test split, but they all side more towards the train set, which is good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making sure the trian and test set have the 75/25 split for each animal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, for each animal, around 75% should be in the training data and around 25% should be in the test data. This ensures neither the train or test set have a unbalanced amount of a certain animal, leading to bias and bad predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of label columns\n",
    "label_columns = ['antelope_duiker', 'bird', 'blank', 'civet_genet', 'hog', 'leopard', 'monkey_prosimian', 'rodent']\n",
    "\n",
    "# Calculate the count of each label in both sets\n",
    "label_counts_train = final_train_set[label_columns].sum()\n",
    "label_counts_test = final_test_set[label_columns].sum()\n",
    "\n",
    "# Combine the counts into a single DataFrame for comparison\n",
    "combined_label_counts = pd.DataFrame({'Train Count': label_counts_train, 'Test Count': label_counts_test})\n",
    "\n",
    "# Calculate the total counts for each label\n",
    "combined_label_counts['Total Count'] = combined_label_counts['Train Count'] + combined_label_counts['Test Count']\n",
    "\n",
    "# Calculate the percentage split for each label\n",
    "combined_label_counts['Train Percentage'] = (combined_label_counts['Train Count'] / combined_label_counts['Total Count']) * 100\n",
    "combined_label_counts['Test Percentage'] = (combined_label_counts['Test Count'] / combined_label_counts['Total Count']) * 100\n",
    "\n",
    "# Display the combined counts with percentage split\n",
    "#print(combined_label_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each class has around a 75/25 split for train and test split, my data looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the data to include only the original columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(final_train_set)\n",
    "\n",
    "# Remove all the new features that I created for the stratified train_test split\n",
    "train_X = final_train_set[['id', 'filepath', 'site']]\n",
    "train_Y = final_train_set[['id','antelope_duiker','bird','blank','civet_genet','hog','leopard','monkey_prosimian','rodent']]\n",
    "\n",
    "test_X = final_test_set[['id', 'filepath', 'site']]\n",
    "test_Y = final_test_set[['id','antelope_duiker','bird','blank','civet_genet','hog','leopard','monkey_prosimian','rodent']]\n",
    "\n",
    "\n",
    "# Make \"id\" the index column\n",
    "train_X.set_index('id', inplace=True) # inplace = True means that it edits the original dataframe, and no new dataframe is created\n",
    "train_Y.set_index('id', inplace=True)\n",
    "\n",
    "test_X.set_index('id', inplace=True)\n",
    "test_Y.set_index('id', inplace=True)\n",
    "\n",
    "#print(test_Y)\n",
    "\n",
    "\n",
    "#print(train_X)\n",
    "#print(train_Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make it so that if there already exists a dataset for train_X, train_Y, and test_X, then we will use those (so that each of my models are trained on the same data, making them deterministic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used old directory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('train_split_X.csv'):\n",
    "    print(\"create new directory\")\n",
    "    train_X.to_csv('train_split_X.csv')\n",
    "    train_Y.to_csv('train_split_Y.csv')\n",
    "    test_X.to_csv('test_split_X.csv')\n",
    "    test_Y.to_csv('test_split_Y.csv')\n",
    "\n",
    "else:\n",
    "    print(\"used old directory\")\n",
    "    train_X = pd.read_csv('train_split_X.csv')\n",
    "    train_Y = pd.read_csv('train_split_Y.csv')\n",
    "    test_X = pd.read_csv('test_split_X.csv')\n",
    "    test_Y = pd.read_csv(\"test_split_Y.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check to see if each image has the same dimensions since that's important for data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import os\n",
    "\n",
    "# def check_image_dimensions(directory):\n",
    "#     image_sizes = {}\n",
    "#     for img_name in os.listdir(directory):\n",
    "#         if not img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.tif')): # now also checks for .tif\n",
    "#             continue\n",
    "#         img_path = os.path.join(directory, img_name)\n",
    "#         with Image.open(img_path) as img:\n",
    "#             # Get image size\n",
    "#             size = img.size\n",
    "#             if size in image_sizes:\n",
    "#                 image_sizes[size] += 1\n",
    "#             else:\n",
    "#                 image_sizes[size] = 1\n",
    "\n",
    "#     for size, count in image_sizes.items():\n",
    "#         print(f\"For the {directory} directory, {count} images are of dimension {size}\")\n",
    "\n",
    "# # Use it on the train and test data only if this code segment was never ran in this coding session:\n",
    "\n",
    "# # Use it on the train and test data:\n",
    "# check_image_dimensions('train_features')\n",
    "# check_image_dimensions('test_features')\n",
    "\n",
    "\n",
    "# # # For the train_features directory, different dimensions found: {(160, 120), (960, 515), (640, 335), (960, 540), (640, 360), (360, 215), (160, 95), (360, 240)}\n",
    "# # # For the test_features directory, different dimensions found: {(960, 515), (160, 120), (640, 335), (960, 540), (320, 215), (640, 360), (360, 240), (320, 240)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resizing all the images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will actually make the resizing function when I use the ImageDataGenertor. \n",
    "# By calling the resize function in the ImageDataGenerator, I won't have to save my images in my local folder and waste space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before I use ImageDataGenerator, I have to format my dataframes so that they are able to be read properly by ImageDataGenertor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a multi-class problem so data format has to be modified so that it can be handled by ImageDataGenerator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             filepath   site  antelope_duiker  bird  blank  \\\n",
      "id                                                                           \n",
      "ZJ000687  train_features/ZJ000687.jpg  S0043              0.0   0.0    0.0   \n",
      "ZJ000926  train_features/ZJ000926.jpg  S0138              1.0   0.0    0.0   \n",
      "ZJ014494  train_features/ZJ014494.jpg  S0122              0.0   0.0    1.0   \n",
      "ZJ007702  train_features/ZJ007702.jpg  S0141              1.0   0.0    0.0   \n",
      "ZJ006294  train_features/ZJ006294.jpg  S0150              0.0   0.0    0.0   \n",
      "...                               ...    ...              ...   ...    ...   \n",
      "ZJ015363  train_features/ZJ015363.jpg  S0176              0.0   0.0    0.0   \n",
      "ZJ015565  train_features/ZJ015565.jpg  S0124              0.0   1.0    0.0   \n",
      "ZJ015926  train_features/ZJ015926.jpg  S0197              1.0   0.0    0.0   \n",
      "ZJ016248  train_features/ZJ016248.jpg  S0092              0.0   0.0    1.0   \n",
      "ZJ016482  train_features/ZJ016482.jpg  S0146              0.0   0.0    1.0   \n",
      "\n",
      "          civet_genet  hog  leopard  monkey_prosimian  rodent  \\\n",
      "id                                                              \n",
      "ZJ000687          0.0  0.0      1.0               0.0     0.0   \n",
      "ZJ000926          0.0  0.0      0.0               0.0     0.0   \n",
      "ZJ014494          0.0  0.0      0.0               0.0     0.0   \n",
      "ZJ007702          0.0  0.0      0.0               0.0     0.0   \n",
      "ZJ006294          0.0  0.0      0.0               1.0     0.0   \n",
      "...               ...  ...      ...               ...     ...   \n",
      "ZJ015363          0.0  0.0      1.0               0.0     0.0   \n",
      "ZJ015565          0.0  0.0      0.0               0.0     0.0   \n",
      "ZJ015926          0.0  0.0      0.0               0.0     0.0   \n",
      "ZJ016248          0.0  0.0      0.0               0.0     0.0   \n",
      "ZJ016482          0.0  0.0      0.0               0.0     0.0   \n",
      "\n",
      "                    labels  \n",
      "id                          \n",
      "ZJ000687           leopard  \n",
      "ZJ000926   antelope_duiker  \n",
      "ZJ014494             blank  \n",
      "ZJ007702   antelope_duiker  \n",
      "ZJ006294  monkey_prosimian  \n",
      "...                    ...  \n",
      "ZJ015363           leopard  \n",
      "ZJ015565              bird  \n",
      "ZJ015926   antelope_duiker  \n",
      "ZJ016248             blank  \n",
      "ZJ016482             blank  \n",
      "\n",
      "[4123 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Merge the two datasets\n",
    "merged_train = pd.merge(train_X, train_Y, on='id')\n",
    "\n",
    "# Set \"id\" as the index column\n",
    "merged_train.set_index(\"id\", inplace = True) # inplace = True means that it edits the original dataframe, and no new dataframe is created\n",
    "\n",
    "# Convert multi-label columns into a single column, so that this column tells us what animal type the row is \n",
    "merged_train['labels'] = merged_train.apply(lambda row: ' '.join([col for col in merged_train.columns[2:] if row[col]==1]), axis=1)\n",
    "\n",
    "\n",
    "# Do the same thing for the test set (from train-test split)\n",
    "\n",
    "\n",
    "\n",
    "# Merge the two datasets\n",
    "merged_test = pd.merge(test_X, test_Y, on='id')\n",
    "\n",
    "# Set \"id\" as the index column\n",
    "merged_test.set_index(\"id\", inplace = True) # inplace = True means that it edits the original dataframe, and no new dataframe is created\n",
    "\n",
    "# Convert multi-label columns into a single column, so that this column tells us what animal type the row is \n",
    "merged_test['labels'] = merged_test.apply(lambda row: ' '.join([col for col in merged_test.columns[2:] if row[col]==1]), axis=1)\n",
    "\n",
    "print(merged_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Keras ImageDataGenerator() on Train/Test split\n",
    "The ImageDataGenerator not only helps you load images from the disk but also allows you to perform **data augmentation**, which is a technique to increase the diversity of your training set by applying random transformations (like rotation, zoom, flips, etc.) to the images. This is very useful to prevent overfitting and helps the model generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12365 validated image filenames belonging to 8 classes.\n",
      "Found 4123 validated image filenames belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Creating an instance of the ImageDataGenerator for data augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Rescale the image pixel values to [0,1]\n",
    "    #preprocessing_function = custom_resize,  # call the crop function on each image\n",
    "\n",
    "    # rotation_range=20,  # Up to 20 degrees of rotation\n",
    "    # brightness_range=[0.8, 1.2],  # Adjust brightness by 20%\n",
    "    # horizontal_flip=True,  # Horizontal flip\n",
    "    # fill_mode='nearest' # The value \"nearest\" for fill_mode means that any empty space will be filled with the nearest pixel values. In other words, it copies the value of the nearest border pixel to fill the gap. \n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Rescale the image pixel values to [0,1]\n",
    "    #preprocessing_function = custom_resize,  # call the crop function on each image\n",
    "\n",
    "\n",
    "    # You not supposed to do data augmetation on test data, to replicate real life data. The only data augmentation should be resizing\n",
    ")\n",
    "\n",
    "# Flow from dataframe method to load images using the dataframe\n",
    "\n",
    "train_split_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=merged_train,\n",
    "    x_col='filepath',\n",
    "    y_col='labels',\n",
    "    target_size=(224, 224),  # automatically resizes so no need for custom resize. When I did custom resize, it made my model worse\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "test_split_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=merged_test, \n",
    "    x_col='filepath',\n",
    "    y_col='labels',\n",
    "    target_size=(224, 224), # automatically resizes so no need for custom resize. When I did custom resize, it made my model worse\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Keras ImageDataGenerator() on my test data (from test_values.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4464 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Rescale the image pixel values to [0,1]\n",
    "    #preprocessing_function = custom_resize,  # call the crop function on each image\n",
    "\n",
    "\n",
    "    # You not supposed to do data augmetation on test data, to replicate real life data. The only data augmentation should be resizing\n",
    ")\n",
    "\n",
    "# Flow from dataframe method to load images using the dataframe\n",
    "\n",
    "\n",
    "test_original_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_values, \n",
    "    x_col='filepath',\n",
    "    #y_col='', # The actual test data has no labels\n",
    "    target_size=(224, 224),   # automatically resizes so no need for custom resize. When I did custom resize, it made my model worse\n",
    "    color_mode='rgb',\n",
    "    class_mode= None, # The actual test data has no labels\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Retrieve a batch of images\n",
    "# images = next(train_split_generator)\n",
    "\n",
    "# # Display the first two images\n",
    "# plt.figure(figsize=(10, 5))\n",
    "\n",
    "# # Image 1\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.imshow(images[0])\n",
    "# plt.title(\"Image 1\")\n",
    "# plt.axis('off')\n",
    "\n",
    "# # Image 2\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.imshow(images[1])\n",
    "# plt.title(\"Image 2\")\n",
    "# plt.axis('off')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# class ImageDataGeneratorWithPaths(ImageDataGenerator):\n",
    "#     def flow_from_dataframe(self, *args, **kwargs):\n",
    "#         base_generator = super().flow_from_dataframe(*args, **kwargs)\n",
    "#         while True:\n",
    "#             batch_data = next(base_generator)\n",
    "#             indices = base_generator.index_array\n",
    "#             filenames = [base_generator.filenames[i] for i in indices]\n",
    "#             yield batch_data[0], batch_data[1], filenames\n",
    "\n",
    "# test_datagen = ImageDataGeneratorWithPaths(\n",
    "#     rescale=1./255,  # Rescale the image pixel values to [0,1]\n",
    "# )\n",
    "\n",
    "# # Flow from dataframe method to load images using the dataframe\n",
    "# test_original_generator = test_datagen.flow_from_dataframe(\n",
    "#     dataframe=test_values, \n",
    "#     x_col='filepath',\n",
    "#     target_size=(224, 224),   \n",
    "#     color_mode='rgb',\n",
    "#     class_mode= None, \n",
    "#     batch_size=32,\n",
    "#     shuffle=False,\n",
    "#     seed=42\n",
    "# )\n",
    "\n",
    "# # Get a batch of images, labels, and filenames\n",
    "# images, labels, filenames = next(test_original_generator)\n",
    "\n",
    "# # Now, filenames contains the filenames of the images in the current batch\n",
    "# print(filenames)\n",
    "\n",
    "# # Display the first two images\n",
    "# plt.figure(figsize=(10, 5))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.imshow(images[0])\n",
    "# plt.title('Image 1')\n",
    "# #plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "# # plt.subplot(1, 2, 2)\n",
    "# # plt.imshow(images[1])\n",
    "# # plt.title('Image 2')\n",
    "# # plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm that the generators have the correct image size of (height,width) = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Get a batch of images and labels from the train generator\n",
    "# images, labels = next(train_split_generator)\n",
    "\n",
    "# # Print the shape, type of the images, and filepaths\n",
    "# print(\"Images shape:\", images.shape)\n",
    "# print(\"Images type:\", images.dtype)\n",
    "# print(\"Filepaths:\", train_split_generator.filenames[:2])  # Print the first two filepaths\n",
    "\n",
    "# print(merged_train)\n",
    "\n",
    "# # Display the first two images\n",
    "# plt.figure(figsize=(10, 5))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.imshow(images[0])\n",
    "# plt.title('Image 1')\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.imshow(images[1])\n",
    "# plt.title('Image 2')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # Do the same for the test generator\n",
    "# images, labels = next(test_split_generator)\n",
    "# print(\"Images shape:\", images.shape)\n",
    "# print(\"Images type:\", images.dtype)\n",
    "# print(\"Filepaths:\", test_split_generator.filenames[:2])  # Print the first two filepaths\n",
    "\n",
    "# # Display the first two images\n",
    "# plt.figure(figsize=(10, 5))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.imshow(images[0])\n",
    "# plt.title('Image 1')\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.imshow(images[1])\n",
    "# plt.title('Image 2')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Development:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 of ResNet50. Score: 2.0596"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 21:26:04.181091: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-01 21:26:04.406986: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-01 21:26:04.407048: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-01 21:26:04.414191: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-01 21:26:04.414248: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-01 21:26:04.414268: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-01 21:26:04.774969: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-01 21:26:04.775023: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-01 21:26:04.775030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-02-01 21:26:04.775060: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-01 21:26:04.775076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5535 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2024-02-01 21:26:08.660599: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-02-01 21:26:09.663644: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fae5af663f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-01 21:26:09.663692: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "2024-02-01 21:26:09.681826: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1706840769.784942     913 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387/387 [==============================] - 132s 334ms/step - loss: 1.9244 - accuracy: 0.2521 - categorical_crossentropy: 1.9244 - val_loss: 1.8092 - val_accuracy: 0.2857 - val_categorical_crossentropy: 1.8092\n",
      "140/140 [==============================] - 35s 250ms/step\n",
      "Successfully submitted!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "# Load pre-trained ResNet50 model without the top layer\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = base_model.output # This line gets the output of the base_model (ResNet50 without the top layer) and uses it as the starting point for adding new layers.\n",
    "x = GlobalAveragePooling2D()(x)  # Add a global spatial average pooling layer\n",
    "x = Dense(1024, activation='relu')(x)  # Add a fully-connected layer\n",
    "predictions = Dense(8, activation='softmax')(x)  # Add a final output layer\n",
    "\n",
    "# This is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions) # this model links the ResNetModel and the new layers to the prediction layer\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', 'categorical_crossentropy'])\n",
    "\n",
    "model.fit(train_split_generator, epochs=1, validation_data=test_split_generator)\n",
    "\n",
    "predictions = model.predict(test_original_generator)\n",
    "\n",
    "# Create a DataFrame with predictions\n",
    "# Create a DataFrame with predictions\n",
    "prediction_df = pd.DataFrame(predictions, columns=['antelope_duiker', 'bird', 'blank', 'civet_genet', 'hog', 'leopard', 'monkey_prosimian', 'rodent'])\n",
    "\n",
    "# Add the 'id' from the original test DataFrame's index\n",
    "prediction_df['id'] = test_values['id'] # id column from the original test set (test_values.csv)\n",
    "\n",
    "# Ensure 'id' is the first column\n",
    "prediction_df = prediction_df[['id'] + [col for col in prediction_df.columns if col != 'id']]\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "prediction_df.to_csv('submission_ResNet50_2.csv', index=False) # make sure to change file name to represent the model number\n",
    "print(\"Successfully submitted!\")\n",
    "\n",
    "# Took 4 minutes for one epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another thing to possibly try is to pick an even smaller 16:9 aspect ratio so that there is little to none upsampling (making images larger) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

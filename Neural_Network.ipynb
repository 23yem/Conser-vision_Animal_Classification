{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to my notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default code from Kaggle Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "   \n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying some important libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow: 2.15.0\n",
      "kerastuner: 1.0.5\n",
      "keras_tuner: 1.3.5\n",
      "Python: 3.10.12\n",
      "numpy: 1.24.3\n",
      "pandas: 2.1.4\n",
      "sklearn version: 1.2.2\n",
      "sklearn path: ['/home/michaelye22/.local/lib/python3.10/site-packages/sklearn']\n",
      "matplotlib: 3.8.2\n",
      "seaborn: 0.13.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow:\", tf.__version__)\n",
    "\n",
    "import kerastuner as kt\n",
    "print(\"kerastuner:\", kt.__version__)\n",
    "\n",
    "import keras_tuner as kt2\n",
    "print(\"keras_tuner:\", kt2.__version__)\n",
    "\n",
    "import platform\n",
    "print(\"Python:\", platform.python_version())\n",
    "\n",
    "import numpy as np\n",
    "print(\"numpy:\", np.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "print(\"pandas:\", pd.__version__)\n",
    "\n",
    "import sklearn\n",
    "print(\"sklearn version:\", sklearn.__version__)\n",
    "\n",
    "import sklearn\n",
    "print(\"sklearn path:\", sklearn.__path__)\n",
    "\n",
    "import matplotlib\n",
    "print(\"matplotlib:\", matplotlib.__version__)\n",
    "\n",
    "import seaborn as sns\n",
    "print(\"seaborn:\", sns.__version__)\n",
    "\n",
    "# On WSL\n",
    "\n",
    "# 2024-01-30 11:17:52.768682: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
    "# 2024-01-30 11:17:53.149956: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
    "# 2024-01-30 11:17:53.150001: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
    "# 2024-01-30 11:17:53.210606: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
    "# 2024-01-30 11:17:53.339576: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
    "# To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
    "# 2024-01-30 11:17:54.568146: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
    "# Tensorflow: 2.15.0\n",
    "# /tmp/ipykernel_3814/2917868046.py:4: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
    "#   import kerastuner as kt\n",
    "# kerastuner: 1.0.5\n",
    "# keras_tuner: 1.3.5\n",
    "# Python: 3.10.12\n",
    "# numpy: 1.24.3\n",
    "# pandas: 2.1.4\n",
    "# sklearn version: 1.2.2\n",
    "# sklearn path: ['/home/michaelye22/.local/lib/python3.10/site-packages/sklearn']\n",
    "# matplotlib: 3.8.2\n",
    "# seaborn: 0.13.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Global random seed to make sure we can replicate any model that we create (no randomness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the training and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_values are the features (X), and train_labels is the target/label (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.read_csv(\"train_features.csv\")\n",
    "train_y = pd.read_csv(\"train_labels.csv\")\n",
    "\n",
    "test_values = pd.read_csv(\"test_features.csv\")\n",
    "\n",
    "# print(\"train labels:\\n\", train_Y.head())\n",
    "\n",
    "# print(\"train values:\\n\", train_X.head())\n",
    "      \n",
    "# print(\"test_values:\\n\", test_values.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check to see if there are any missing values in the data. If so, we have to do imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in train_X: 0\n",
      "Number of missing values in train_Y: 0\n",
      "Number of missing values in test_values: 0\n"
     ]
    }
   ],
   "source": [
    "missing_train_X = train_X.isnull().sum().sum()\n",
    "print(\"Number of missing values in train_X:\", missing_train_X)\n",
    "\n",
    "missing_train_y = train_y.isnull().sum().sum()\n",
    "print(\"Number of missing values in train_Y:\", missing_train_y)\n",
    "\n",
    "missing_test_values = test_values.isnull().sum().sum()\n",
    "print(\"Number of missing values in test_values:\", missing_test_values )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have 0 missing values in each dataframe, we don't have to do imputation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified train_test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Train Distribution  Test Distribution\n",
      "site_animal                                                  \n",
      "S0001_bird                        0.000647           0.000728\n",
      "S0001_blank                       0.000404           0.000243\n",
      "S0001_leopard                     0.003073           0.003153\n",
      "S0001_monkey_prosimian            0.001051           0.000970\n",
      "S0002_bird                        0.000566           0.000485\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the datasets\n",
    "\n",
    "train_features = train_X\n",
    "train_labels = train_y\n",
    "\n",
    "# Merge the datasets on 'id'\n",
    "merged_data = pd.merge(train_features, train_labels, on='id')\n",
    "\n",
    "# Identifying the animal present in each image and creating a combined category\n",
    "animal_columns = ['antelope_duiker', 'bird', 'blank', 'civet_genet', 'hog', 'leopard', 'monkey_prosimian', 'rodent']\n",
    "merged_data['animal'] = merged_data[animal_columns].idxmax(axis=1)\n",
    "merged_data['site_animal'] = merged_data['site'] + '_' + merged_data['animal']\n",
    "\n",
    "# Checking the number of instances for each site_animal combination\n",
    "combination_counts = merged_data['site_animal'].value_counts()\n",
    "rare_combinations = combination_counts[combination_counts < 5]\n",
    "\n",
    "# Separating the dataset into common and rare combinations\n",
    "common_combinations = merged_data[~merged_data['site_animal'].isin(rare_combinations.index)]\n",
    "rare_combinations_data = merged_data[merged_data['site_animal'].isin(rare_combinations.index)]\n",
    "\n",
    "# Stratified split for common combinations\n",
    "common_train_set, common_test_set = train_test_split(\n",
    "    common_combinations, test_size=0.25, stratify=common_combinations['site_animal'], random_state=42)\n",
    "\n",
    "# Randomly splitting rare combinations\n",
    "total_samples = rare_combinations_data.shape[0]\n",
    "train_samples = int(np.round(total_samples * 0.75))\n",
    "rare_train_set = rare_combinations_data.sample(n=train_samples, random_state=42)\n",
    "rare_test_set = rare_combinations_data.drop(rare_train_set.index)\n",
    "\n",
    "# Combining the splits into final train and test sets\n",
    "final_train_set = pd.concat([common_train_set, rare_train_set])\n",
    "final_test_set = pd.concat([common_test_set, rare_test_set])\n",
    "\n",
    "# Optional: Verifying the final distribution (can be commented out for large datasets)\n",
    "final_train_distribution = final_train_set['site_animal'].value_counts(normalize=True)\n",
    "final_test_distribution = final_test_set['site_animal'].value_counts(normalize=True)\n",
    "final_distribution_summary = pd.DataFrame({\n",
    "    'Train Distribution': final_train_distribution,\n",
    "    'Test Distribution': final_test_distribution\n",
    "})\n",
    "print(final_distribution_summary.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the 75/25 train_test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Proportion: 74.99%\n",
      "Test Set Proportion: 25.01%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of samples in each set\n",
    "num_train_samples = final_train_set.shape[0]\n",
    "num_test_samples = final_test_set.shape[0]\n",
    "total_samples = num_train_samples + num_test_samples\n",
    "\n",
    "# Calculate the proportions\n",
    "train_proportion = num_train_samples / total_samples\n",
    "test_proportion = num_test_samples / total_samples\n",
    "\n",
    "# Print out the proportions\n",
    "print(\"Training Set Proportion: {:.2%}\".format(train_proportion))\n",
    "print(\"Test Set Proportion: {:.2%}\".format(test_proportion))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making sure that train and test set have a 75/25 split for each site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, for each site, around 75% should be in the training data and around 25% should be in the test data. This ensures niether the train or test set have a unbalanced amount of a certain site, leading to bias and bad predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the count of each site in both sets\n",
    "site_counts_train = final_train_set['site'].value_counts()\n",
    "site_counts_test = final_test_set['site'].value_counts()\n",
    "\n",
    "# Combine the counts into a single DataFrame for comparison\n",
    "combined_site_counts = pd.DataFrame({'Train Count': site_counts_train, 'Test Count': site_counts_test})\n",
    "\n",
    "# Calculate the total counts for each site\n",
    "combined_site_counts['Total Count'] = combined_site_counts['Train Count'] + combined_site_counts['Test Count']\n",
    "\n",
    "# Calculate the percentage split for each site\n",
    "combined_site_counts['Train Percentage'] = (combined_site_counts['Train Count'] / combined_site_counts['Total Count']) * 100\n",
    "combined_site_counts['Test Percentage'] = (combined_site_counts['Test Count'] / combined_site_counts['Total Count']) * 100\n",
    "\n",
    "# Display the combined counts with percentage split\n",
    "#print(combined_site_counts.head())\n",
    "combined_site_counts.to_csv(\"site_percentage_check.csv\")\n",
    "\n",
    "# Check to see if there any rows of data with a train percentage below 70 or above 80 (the ideal is 75)\n",
    "filtered_df = combined_site_counts[(combined_site_counts['Train Percentage'] < 70) | (combined_site_counts['Train Percentage'] > 80)]\n",
    "#print(filtered_df)\n",
    "#print(len(filtered_df)) # There are only 13 sites which have a bad train/test split, but they all side more towards the train set, which is good\n",
    "\n",
    "\n",
    "#print(len(final_train_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 13 sites which have a bad train/test split, but they all side more towards the train set, which is good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making sure the trian and test set have the 75/25 split for each animal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, for each animal, around 75% should be in the training data and around 25% should be in the test data. This ensures neither the train or test set have a unbalanced amount of a certain animal, leading to bias and bad predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of label columns\n",
    "label_columns = ['antelope_duiker', 'bird', 'blank', 'civet_genet', 'hog', 'leopard', 'monkey_prosimian', 'rodent']\n",
    "\n",
    "# Calculate the count of each label in both sets\n",
    "label_counts_train = final_train_set[label_columns].sum()\n",
    "label_counts_test = final_test_set[label_columns].sum()\n",
    "\n",
    "# Combine the counts into a single DataFrame for comparison\n",
    "combined_label_counts = pd.DataFrame({'Train Count': label_counts_train, 'Test Count': label_counts_test})\n",
    "\n",
    "# Calculate the total counts for each label\n",
    "combined_label_counts['Total Count'] = combined_label_counts['Train Count'] + combined_label_counts['Test Count']\n",
    "\n",
    "# Calculate the percentage split for each label\n",
    "combined_label_counts['Train Percentage'] = (combined_label_counts['Train Count'] / combined_label_counts['Total Count']) * 100\n",
    "combined_label_counts['Test Percentage'] = (combined_label_counts['Test Count'] / combined_label_counts['Total Count']) * 100\n",
    "\n",
    "# Display the combined counts with percentage split\n",
    "#print(combined_label_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each class has around a 75/25 split for train and test split, my data looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the data to include only the original columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(final_train_set)\n",
    "\n",
    "# Remove all the new features that I created for the stratified train_test split\n",
    "train_X = final_train_set[['id', 'filepath', 'site']]\n",
    "train_Y = final_train_set[['id','antelope_duiker','bird','blank','civet_genet','hog','leopard','monkey_prosimian','rodent']]\n",
    "\n",
    "test_X = final_test_set[['id', 'filepath', 'site']]\n",
    "test_Y = final_test_set[['id','antelope_duiker','bird','blank','civet_genet','hog','leopard','monkey_prosimian','rodent']]\n",
    "\n",
    "\n",
    "# Make \"id\" the index column\n",
    "train_X.set_index('id', inplace=True)\n",
    "train_Y.set_index('id', inplace=True)\n",
    "\n",
    "test_X.set_index('id', inplace=True)\n",
    "test_Y.set_index('id', inplace=True)\n",
    "\n",
    "#print(test_Y)\n",
    "\n",
    "\n",
    "#print(train_X)\n",
    "#print(train_Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make it so that if there already exists a dataset for train_X, train_Y, and test_X, then we will use those (so that each of my models are trained on the same data, making them deterministic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create new directory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('train_split_X.csv'):\n",
    "    print(\"create new directory\")\n",
    "    train_X.to_csv('train_split_X.csv')\n",
    "    train_Y.to_csv('train_split_Y.csv')\n",
    "    test_X.to_csv('test_split_X.csv')\n",
    "    test_Y.to_csv('test_split_Y.csv')\n",
    "\n",
    "else:\n",
    "    print(\"used old directory\")\n",
    "    train_X = pd.read_csv('train_X.csv')\n",
    "    train_Y = pd.read_csv('train_Y.csv')\n",
    "    test_X = pd.read_csv('test_X.csv')\n",
    "    test_Y = pd.read_csv(\"test_split.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check to see if each image has the same dimensions since that's important for data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import os\n",
    "\n",
    "# def check_image_dimensions(directory):\n",
    "#     image_sizes = {}\n",
    "#     for img_name in os.listdir(directory):\n",
    "#         if not img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.tif')): # now also checks for .tif\n",
    "#             continue\n",
    "#         img_path = os.path.join(directory, img_name)\n",
    "#         with Image.open(img_path) as img:\n",
    "#             # Get image size\n",
    "#             size = img.size\n",
    "#             if size in image_sizes:\n",
    "#                 image_sizes[size] += 1\n",
    "#             else:\n",
    "#                 image_sizes[size] = 1\n",
    "\n",
    "#     for size, count in image_sizes.items():\n",
    "#         print(f\"For the {directory} directory, {count} images are of dimension {size}\")\n",
    "\n",
    "# # Use it on the train and test data only if this code segment was never ran in this coding session:\n",
    "\n",
    "# # Use it on the train and test data:\n",
    "# check_image_dimensions('train_features')\n",
    "# check_image_dimensions('test_features')\n",
    "\n",
    "\n",
    "# # # For the train_features directory, different dimensions found: {(160, 120), (960, 515), (640, 335), (960, 540), (640, 360), (360, 215), (160, 95), (360, 240)}\n",
    "# # # For the test_features directory, different dimensions found: {(960, 515), (160, 120), (640, 335), (960, 540), (320, 215), (640, 360), (360, 240), (320, 240)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resizing all the images to (640, 360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know the main sizes are (960, 540) and (640, 360). LUCKILY, they both close to a 16:9 aspect ratio, so both are viable options. However, it is generally better to shrink than to enlarge, so I'm choosing (640, 360).\n",
    "\n",
    "Another thing to possibly try is to pick an even smaller 16:9 aspect ratio so that there is little to none upsampling (making images larger) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will actually make the resizing function when I use the ImageDataGenertor. \n",
    "# By calling the resize function in the ImageDataGenerator, I won't have to save my images in my local folder and waste space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Keras ImageDataGenerator() on Train/Test split\n",
    "The ImageDataGenerator not only helps you load images from the disk but also allows you to perform **data augmentation**, which is a technique to increase the diversity of your training set by applying random transformations (like rotation, zoom, flips, etc.) to the images. This is very useful to prevent overfitting and helps the model generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 46\u001b[0m\n\u001b[1;32m     40\u001b[0m test_datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(rescale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m, preprocessing_function \u001b[38;5;241m=\u001b[39m custom_resize) \u001b[38;5;66;03m# call the crop function on each image\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Flow from dataframe method to load images using the dataframe\u001b[39;00m\n\u001b[1;32m     45\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m train_datagen\u001b[38;5;241m.\u001b[39mflow_from_dataframe(\n\u001b[0;32m---> 46\u001b[0m     dataframe\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_df\u001b[49m, \u001b[38;5;66;03m# Use the training dataframe (with labels, id, and paths)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     x_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     48\u001b[0m     y_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     49\u001b[0m     target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m),  \u001b[38;5;66;03m# The dimensions to which all images found will be resized. Change this as needed\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     color_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     51\u001b[0m     class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# means that the labels are binary labels\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     53\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# This might introduce randomness if set to true, but if it's false, the it might lead to overfitting. So it's best to just save the neural network to ensure no randomness\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     55\u001b[0m )\n\u001b[1;32m     57\u001b[0m val_generator \u001b[38;5;241m=\u001b[39m val_datagen\u001b[38;5;241m.\u001b[39mflow_from_dataframe(\n\u001b[1;32m     58\u001b[0m     dataframe\u001b[38;5;241m=\u001b[39mval_df, \u001b[38;5;66;03m# Use the validation dataframe (with labels, id, and paths)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     x_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     67\u001b[0m )\n\u001b[1;32m     69\u001b[0m test_generator \u001b[38;5;241m=\u001b[39m test_datagen\u001b[38;5;241m.\u001b[39mflow_from_dataframe(\n\u001b[1;32m     70\u001b[0m     dataframe\u001b[38;5;241m=\u001b[39mtest_df, \u001b[38;5;66;03m# Use the testing dataframe (with labels, id, and paths)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     x_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     79\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def custom_resize(image, target_size=(640, 360)):\n",
    "    # Convert the input NumPy array image to a PIL Image\n",
    "    img = Image.fromarray(image)\n",
    "\n",
    "    # Resize the image\n",
    "    img_resized = img.resize(target_size, Image.ANTIALIAS)\n",
    "\n",
    "    # Convert back to NumPy array and return\n",
    "    return np.array(img_resized)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Creating an instance of the ImageDataGenerator for data augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Rescale the image pixel values to [0,1]\n",
    "    preprocessing_function = custom_resize  # call the crop function on each image\n",
    "\n",
    "    # Potential data augmentation techniques that won't affect the 32x32px center\n",
    "    #brightness_range=[0.8, 1.2], \n",
    "    #channel_shift_range=20, \n",
    "\n",
    "    # I removed these transformations for the data augmentation since this project involves detecting tumor tissue in the center 32x32px region so I can't be doing zooming and other transformations for this project specifically\n",
    "\n",
    "    # rotation_range=40,  # Random rotations\n",
    "    # width_shift_range=0.2,  # Random horizontal shifts\n",
    "    # height_shift_range=0.2,  # Random vertical shifts\n",
    "    # shear_range=0.2,  # Shear transformations\n",
    "    # zoom_range=0.2,  # Random zoom\n",
    "    # horizontal_flip=True,  # Random horizontal flips\n",
    "    # fill_mode='nearest'  # Strategy for filling in new pixels\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale = 1./255, preprocessing_function = custom_resize)  # call the crop function on each image\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255, preprocessing_function = custom_resize) # call the crop function on each image\n",
    "\n",
    "\n",
    "\n",
    "# Flow from dataframe method to load images using the dataframe\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df, # Use the training dataframe (with labels, id, and paths)\n",
    "    x_col='path',\n",
    "    y_col='label',\n",
    "    target_size=(32, 32),  # The dimensions to which all images found will be resized. Change this as needed\n",
    "    color_mode='rgb',\n",
    "    class_mode='binary', # means that the labels are binary labels\n",
    "    batch_size=32,\n",
    "    shuffle=True, # This might introduce randomness if set to true, but if it's false, the it might lead to overfitting. So it's best to just save the neural network to ensure no randomness\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    dataframe=val_df, # Use the validation dataframe (with labels, id, and paths)\n",
    "    x_col='path',\n",
    "    y_col='label',\n",
    "    target_size=(32, 32),\n",
    "    color_mode='rgb',\n",
    "    class_mode='binary',\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_df, # Use the testing dataframe (with labels, id, and paths)\n",
    "    x_col='path',\n",
    "    y_col='label',\n",
    "    target_size=(32, 32),\n",
    "    color_mode='rgb',\n",
    "    class_mode='binary',\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# After setting this up, you can use train_generator as the input to the fit or fit_generator method of your Keras model, \n",
    "# which will load images in batches and train your model on them.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another thing to possibly try is to pick an even smaller 16:9 aspect ratio so that there is little to none upsampling (making images larger) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
